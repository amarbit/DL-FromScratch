{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AdaGrad Optimizer from Scratch\n",
        "\n",
        "This notebook implements the AdaGrad (Adaptive Gradient) optimizer from scratch using only NumPy, including support for learning rate, numerical stability (eps), and optional weight decay (L2 regularization).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AdaGrad Optimizer Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AdaGrad:\n",
        "    \"\"\"\n",
        "    AdaGrad (Adaptive Gradient) optimizer implemented from scratch using NumPy.\n",
        "    \n",
        "    AdaGrad adapts the learning rate for each parameter by accumulating the squared gradients\n",
        "    throughout training. Parameters with large accumulated squared gradients will have smaller\n",
        "    learning rates, while parameters with small accumulated squared gradients will have relatively\n",
        "    larger learning rates.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    lr : float, optional\n",
        "        Learning rate (step size) for weight updates (default: 0.01)\n",
        "    eps : float, optional\n",
        "        Small constant added to the denominator for numerical stability (default: 1e-8)\n",
        "    weight_decay : float, optional\n",
        "        L2 regularization coefficient (default: 0.0)\n",
        "    \n",
        "    Algorithm:\n",
        "    ----------\n",
        "    1. Accumulate squared gradients: G_t = G_{t-1} + g_t^2\n",
        "    2. Update parameters: w_t = w_{t-1} - lr * g_t / (√G_t + ε)\n",
        "    \n",
        "    Key Characteristics:\n",
        "    --------------------\n",
        "    - Accumulates squared gradients from the beginning of training (no decay)\n",
        "    - Learning rates decrease monotonically over time\n",
        "    - Works well for sparse gradients and convex problems\n",
        "    - Can lead to vanishing learning rates in the long run\n",
        "    \n",
        "    Examples:\n",
        "    --------\n",
        "    >>> optimizer = AdaGrad(lr=0.01, eps=1e-8, weight_decay=0.01)\n",
        "    >>> optimizer.step(weights, gradients)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, lr=0.01, eps=1e-8, weight_decay=0.0):\n",
        "        self.lr = lr\n",
        "        self.eps = eps\n",
        "        self.weight_decay = weight_decay\n",
        "        \n",
        "        # Dictionary to store accumulated squared gradients for each parameter set\n",
        "        self.G = {}\n",
        "    \n",
        "    def step(self, params, grads):\n",
        "        \"\"\"\n",
        "        Perform a single AdaGrad optimization step.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        params : numpy.ndarray\n",
        "            Model parameters (weights) to be updated (modified in-place)\n",
        "        grads : numpy.ndarray\n",
        "            Gradients with respect to the parameters\n",
        "        \n",
        "        Algorithm:\n",
        "        ----------\n",
        "        1. Apply weight decay if enabled: g_t = g_t + weight_decay * w_t\n",
        "        2. Accumulate squared gradients: G_t = G_{t-1} + g_t^2\n",
        "        3. Update parameters: w_t = w_{t-1} - lr * g_t / (√G_t + ε)\n",
        "        \"\"\"\n",
        "        # Get unique identifier for this parameter set\n",
        "        param_id = id(params)\n",
        "        \n",
        "        # Initialize accumulated squared gradients if not exists\n",
        "        if param_id not in self.G:\n",
        "            self.G[param_id] = np.zeros_like(params)\n",
        "        \n",
        "        # Apply L2 regularization (weight decay) to gradients\n",
        "        if self.weight_decay > 0:\n",
        "            grads = grads + self.weight_decay * params\n",
        "        \n",
        "        # Accumulate squared gradients\n",
        "        # G_t = G_{t-1} + g_t^2\n",
        "        self.G[param_id] = self.G[param_id] + (grads ** 2)\n",
        "        \n",
        "        # Update parameters\n",
        "        # w_t = w_{t-1} - lr * g_t / (√G_t + ε)\n",
        "        params[:] = params - self.lr * grads / (np.sqrt(self.G[param_id]) + self.eps)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Numerical Example: Parameter Updates Over Several Steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple numerical example\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "W = np.random.randn(2, 2)\n",
        "grads = np.random.randn(2, 2)\n",
        "\n",
        "print(\"Initial weights W:\")\n",
        "print(W)\n",
        "print(\"\\nInitial gradients:\")\n",
        "print(grads)\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# Create AdaGrad optimizer\n",
        "adagrad = AdaGrad(lr=0.01, eps=1e-8)\n",
        "\n",
        "print(\"AdaGrad optimizer parameters:\")\n",
        "print(f\"  Learning rate: {adagrad.lr}\")\n",
        "print(f\"  Epsilon: {adagrad.eps}\")\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# Perform 5 optimization steps\n",
        "for i in range(5):\n",
        "    adagrad.step(W, grads)\n",
        "    print(f\"Step {i+1}, Updated Weights:\\n{W}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detailed Example: Showing Intermediate Values (G_t and Adaptive Learning Rates)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed example showing intermediate values (G_t and adaptive step sizes)\n",
        "np.random.seed(42)\n",
        "W = np.random.randn(2, 2)\n",
        "grads = np.random.randn(2, 2)\n",
        "\n",
        "print(\"Initial weights W:\")\n",
        "print(W)\n",
        "print(\"\\nInitial gradients:\")\n",
        "print(grads)\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "adagrad = AdaGrad(lr=0.01, eps=1e-8)\n",
        "\n",
        "param_id = id(W)\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"Step {i+1}:\")\n",
        "    print(f\"  Learning rate: {adagrad.lr}\")\n",
        "    print(f\"  Eps: {adagrad.eps}\")\n",
        "    \n",
        "    # Store previous values for display\n",
        "    G_prev = adagrad.G.get(param_id, np.zeros_like(W)).copy()\n",
        "    W_prev = W.copy()\n",
        "    \n",
        "    print(f\"\\n  Before step():\")\n",
        "    print(f\"    G_{i} (accumulated squared gradients): {G_prev}\")\n",
        "    print(f\"    W_{i}: {W_prev}\")\n",
        "    print(f\"    Current gradients: {grads}\")\n",
        "    print(f\"    Gradient magnitudes: {np.abs(grads)}\")\n",
        "    \n",
        "    # Perform step\n",
        "    adagrad.step(W, grads)\n",
        "    \n",
        "    # Display intermediate values\n",
        "    print(f\"\\n  After step() - Updated accumulated squared gradients:\")\n",
        "    print(f\"    G_{i+1} = G_{i} + g_{i+1}^2\")\n",
        "    print(f\"    G_{i+1} = {G_prev} + {grads**2}\")\n",
        "    print(f\"    G_{i+1}: {adagrad.G[param_id]}\")\n",
        "    \n",
        "    # Calculate adaptive step sizes\n",
        "    sqrt_G = np.sqrt(adagrad.G[param_id])\n",
        "    denominator = sqrt_G + adagrad.eps\n",
        "    adaptive_lr = adagrad.lr / denominator\n",
        "    step_size = adaptive_lr * grads\n",
        "    \n",
        "    print(f\"\\n  Adaptive step size calculation:\")\n",
        "    print(f\"    sqrt(G_{i+1}): {sqrt_G}\")\n",
        "    print(f\"    sqrt(G_{i+1}) + eps: {denominator}\")\n",
        "    print(f\"    Adaptive learning rate per parameter: {adagrad.lr} / {denominator}\")\n",
        "    print(f\"    Adaptive learning rate: {adaptive_lr}\")\n",
        "    print(f\"    Step size: lr * g / (sqrt(G) + eps) = {step_size}\")\n",
        "    \n",
        "    print(f\"\\n  Parameter update:\")\n",
        "    print(f\"    W_{i+1} = W_{i} - lr * g_{i+1} / (sqrt(G_{i+1}) + eps)\")\n",
        "    print(f\"    Updated W_{i+1}: {W}\")\n",
        "    print(f\"    Change in W: {W - W_prev}\")\n",
        "    print(\"-\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example: Demonstrating Decreasing Learning Rates Over Time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate how AdaGrad's learning rate decreases over time\n",
        "np.random.seed(42)\n",
        "\n",
        "W = np.random.randn(2, 2)\n",
        "grads = np.random.randn(2, 2)\n",
        "\n",
        "print(\"Demonstrating decreasing learning rates:\")\n",
        "print(f\"Initial weights W:\\n{W}\")\n",
        "print(f\"\\nGradients (constant for demonstration):\\n{grads}\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "adagrad = AdaGrad(lr=0.1, eps=1e-8)  # Use larger lr to see the effect\n",
        "\n",
        "for i in range(5):\n",
        "    G_before = adagrad.G.get(id(W), np.zeros_like(W)).copy()\n",
        "    W_before = W.copy()\n",
        "    \n",
        "    # Perform step\n",
        "    adagrad.step(W, grads)\n",
        "    \n",
        "    # Calculate adaptive learning rates\n",
        "    sqrt_G = np.sqrt(adagrad.G[id(W)])\n",
        "    adaptive_lr = adagrad.lr / (sqrt_G + adagrad.eps)\n",
        "    step_size = adaptive_lr * grads\n",
        "    \n",
        "    print(f\"Step {i+1}:\")\n",
        "    print(f\"  G (accumulated squared gradients):\\n{adagrad.G[id(W)]}\")\n",
        "    print(f\"  sqrt(G):\\n{sqrt_G}\")\n",
        "    print(f\"  Adaptive learning rate per parameter:\\n{adaptive_lr}\")\n",
        "    print(f\"  Step size per parameter:\\n{step_size}\")\n",
        "    print(f\"  Updated W:\\n{W}\")\n",
        "    print(f\"  Change in W:\\n{W - W_before}\")\n",
        "    print(f\"\\n  Key observation: Learning rate decreases as G accumulates!\")\n",
        "    print(f\"  Average adaptive learning rate: {np.mean(adaptive_lr):.6f}\")\n",
        "    print(\"-\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example: AdaGrad with Weight Decay\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example demonstrating AdaGrad with weight decay\n",
        "np.random.seed(42)\n",
        "\n",
        "W = np.random.randn(2, 2)\n",
        "grads = np.random.randn(2, 2)\n",
        "\n",
        "print(\"AdaGrad without weight decay:\")\n",
        "adagrad_no_decay = AdaGrad(lr=0.01, eps=1e-8, weight_decay=0.0)\n",
        "W_no_decay = W.copy()\n",
        "\n",
        "for i in range(3):\n",
        "    adagrad_no_decay.step(W_no_decay, grads)\n",
        "    print(f\"Step {i+1}, W:\\n{W_no_decay}\\n\")\n",
        "\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "print(\"AdaGrad with weight decay (weight_decay=0.01):\")\n",
        "adagrad_with_decay = AdaGrad(lr=0.01, eps=1e-8, weight_decay=0.01)\n",
        "W_with_decay = W.copy()\n",
        "\n",
        "for i in range(3):\n",
        "    adagrad_with_decay.step(W_with_decay, grads)\n",
        "    print(f\"Step {i+1}, W:\\n{W_with_decay}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example: Comparing AdaGrad with Different Learning Rates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare AdaGrad with different learning rates\n",
        "np.random.seed(42)\n",
        "\n",
        "W_1 = np.random.randn(2, 2)\n",
        "W_2 = np.random.randn(2, 2)\n",
        "grads = np.random.randn(2, 2)\n",
        "\n",
        "print(\"AdaGrad with lr=0.01:\")\n",
        "adagrad1 = AdaGrad(lr=0.01, eps=1e-8)\n",
        "for i in range(3):\n",
        "    adagrad1.step(W_1, grads)\n",
        "    print(f\"Step {i+1}: {W_1}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "print(\"AdaGrad with lr=0.1:\")\n",
        "adagrad2 = AdaGrad(lr=0.1, eps=1e-8)\n",
        "for i in range(3):\n",
        "    adagrad2.step(W_2, grads)\n",
        "    print(f\"Step {i+1}: {W_2}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step-by-Step Explanation\n",
        "\n",
        "### AdaGrad Algorithm Overview\n",
        "\n",
        "AdaGrad (Adaptive Gradient) is an adaptive learning rate optimization algorithm that adapts the learning rate for each parameter by accumulating squared gradients throughout training. Unlike RMSProp, AdaGrad accumulates gradients from the beginning without decay.\n",
        "\n",
        "### Key Components:\n",
        "\n",
        "1. **Accumulated Squared Gradients**:\n",
        "   - $G_t = G_{t-1} + g_t^2$\n",
        "   - Accumulates squared gradients from the start of training (no decay factor)\n",
        "   - Tracks the total magnitude of gradients seen so far\n",
        "\n",
        "2. **Parameter Update**:\n",
        "   - $\\theta_t = \\theta_{t-1} - \\alpha \\frac{g_t}{\\sqrt{G_t} + \\epsilon}$\n",
        "   - Where $\\alpha$ is the learning rate and $\\epsilon$ is a small constant for numerical stability\n",
        "   - Divides gradients by the square root of accumulated squared gradients\n",
        "\n",
        "3. **Weight Decay (L2 Regularization)**: If enabled, adds penalty to gradients:\n",
        "   - $g_t = g_t + \\text{weight\\_decay} \\times \\theta_t$\n",
        "\n",
        "### Advantages:\n",
        "\n",
        "- **Automatic Learning Rate Tuning**: No manual tuning needed for each parameter\n",
        "- **Works Well with Sparse Gradients**: Effective for sparse data where gradients are zero most of the time\n",
        "- **Good for Convex Problems**: Performs well on convex optimization problems\n",
        "- **Smaller Steps for Frequent Features**: Parameters that receive large gradients frequently get smaller learning rates\n",
        "\n",
        "### Limitations:\n",
        "\n",
        "- **Monotonically Decreasing Learning Rate**: As $G_t$ accumulates, the learning rate keeps decreasing\n",
        "- **Vanishing Learning Rate Problem**: In the long run, learning rates can become too small to make meaningful progress\n",
        "- **Not Ideal for Non-Convex Problems**: Later variants (RMSProp, Adam) perform better for deep learning\n",
        "\n",
        "### Key Insight:\n",
        "\n",
        "The algorithm divides the gradient by the square root of accumulated squared gradients. This means:\n",
        "- Parameters with consistently large gradients will have large $G_t$, resulting in smaller step sizes\n",
        "- Parameters with small or infrequent gradients will have smaller $G_t$, allowing relatively larger step sizes\n",
        "- Over time, the learning rate decreases for all parameters, which can lead to premature convergence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visual Comparison: AdaGrad vs RMSProp Behavior\n",
        "\n",
        "Note: This cell demonstrates the conceptual difference between AdaGrad (accumulates forever) \n",
        "and RMSProp (exponential moving average with decay). AdaGrad's G keeps growing, while \n",
        "RMSProp's v stabilizes due to the decay factor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate the key difference: AdaGrad accumulates forever (no decay)\n",
        "np.random.seed(42)\n",
        "\n",
        "W = np.random.randn(2, 2)\n",
        "grads = np.random.randn(2, 2)\n",
        "\n",
        "print(\"AdaGrad - Accumulated squared gradients keep growing:\")\n",
        "print(\"=\"*70)\n",
        "adagrad = AdaGrad(lr=0.1, eps=1e-8)\n",
        "\n",
        "for i in range(5):\n",
        "    adagrad.step(W, grads)\n",
        "    G_mean = np.mean(adagrad.G[id(W)])\n",
        "    sqrt_G_mean = np.mean(np.sqrt(adagrad.G[id(W)]))\n",
        "    adaptive_lr_mean = np.mean(adagrad.lr / (np.sqrt(adagrad.G[id(W)]) + adagrad.eps))\n",
        "    print(f\"  Step {i+1}: Mean(G) = {G_mean:.6f}, Mean(√G) = {sqrt_G_mean:.6f}, Mean(adaptive_lr) = {adaptive_lr_mean:.6f}\")\n",
        "\n",
        "print(\"\\nKey observation: G accumulates continuously, causing adaptive learning rate to decrease over time.\")\n",
        "print(\"This is why RMSProp (with decay) was developed to address AdaGrad's vanishing learning rate problem.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
