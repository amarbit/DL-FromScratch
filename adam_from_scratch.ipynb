{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Adam Optimizer (Adaptive Moment Estimation) from Scratch\n",
        "\n",
        "This notebook implements the Adam optimizer from scratch using only NumPy, including bias correction and optional weight decay (L2 regularization).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adam Optimizer Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Adam:\n",
        "    \"\"\"\n",
        "    Adam (Adaptive Moment Estimation) optimizer implemented from scratch using NumPy.\n",
        "    \n",
        "    Adam combines the advantages of two other extensions of stochastic gradient descent:\n",
        "    - Adaptive Gradient Algorithm (AdaGrad) that maintains per-parameter learning rates\n",
        "    - Root Mean Square Propagation (RMSProp) that maintains a moving average of squared gradients\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    lr : float, optional\n",
        "        Learning rate (step size) for weight updates (default: 0.001)\n",
        "    beta1 : float, optional\n",
        "        Exponential decay rate for the first moment estimates (default: 0.9)\n",
        "    beta2 : float, optional\n",
        "        Exponential decay rate for the second moment estimates (default: 0.999)\n",
        "    eps : float, optional\n",
        "        Small constant for numerical stability (default: 1e-8)\n",
        "    weight_decay : float, optional\n",
        "        L2 regularization coefficient (default: 0.0)\n",
        "    \n",
        "    Examples:\n",
        "    --------\n",
        "    >>> optimizer = Adam(lr=0.01, beta1=0.9, beta2=0.999, weight_decay=0.01)\n",
        "    >>> optimizer.step(weights, gradients)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.0):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "        self.weight_decay = weight_decay\n",
        "        \n",
        "        # Dictionary to store first moment (momentum) for each parameter set\n",
        "        self.m = {}\n",
        "        # Dictionary to store second moment (uncentered variance) for each parameter set\n",
        "        self.v = {}\n",
        "        # Dictionary to store step count for bias correction\n",
        "        self.t = {}\n",
        "    \n",
        "    def step(self, params, grads):\n",
        "        \"\"\"\n",
        "        Perform a single Adam optimization step.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        params : numpy.ndarray\n",
        "            Model parameters (weights) to be updated (modified in-place)\n",
        "        grads : numpy.ndarray\n",
        "            Gradients with respect to the parameters\n",
        "        \n",
        "        Algorithm:\n",
        "        ----------\n",
        "        1. Apply weight decay if enabled: g_t = g_t + weight_decay * w_t\n",
        "        2. Update biased first moment estimate: m_t = beta1 * m_{t-1} + (1 - beta1) * g_t\n",
        "        3. Update biased second raw moment estimate: v_t = beta2 * v_{t-1} + (1 - beta2) * g_t^2\n",
        "        4. Compute bias-corrected first moment: m_hat_t = m_t / (1 - beta1^t)\n",
        "        5. Compute bias-corrected second moment: v_hat_t = v_t / (1 - beta2^t)\n",
        "        6. Update parameters: w_t = w_{t-1} - lr * m_hat_t / (sqrt(v_hat_t) + eps)\n",
        "        \"\"\"\n",
        "        # Get unique identifier for this parameter set\n",
        "        param_id = id(params)\n",
        "        \n",
        "        # Initialize moment estimates and step count if not exists\n",
        "        if param_id not in self.m:\n",
        "            self.m[param_id] = np.zeros_like(params)\n",
        "            self.v[param_id] = np.zeros_like(params)\n",
        "            self.t[param_id] = 0\n",
        "        \n",
        "        # Increment step count\n",
        "        self.t[param_id] += 1\n",
        "        t = self.t[param_id]\n",
        "        \n",
        "        # Apply L2 regularization (weight decay) to gradients\n",
        "        if self.weight_decay > 0:\n",
        "            grads = grads + self.weight_decay * params\n",
        "        \n",
        "        # Update biased first moment estimate\n",
        "        # m_t = beta1 * m_{t-1} + (1 - beta1) * g_t\n",
        "        self.m[param_id] = self.beta1 * self.m[param_id] + (1 - self.beta1) * grads\n",
        "        \n",
        "        # Update biased second raw moment estimate\n",
        "        # v_t = beta2 * v_{t-1} + (1 - beta2) * g_t^2\n",
        "        self.v[param_id] = self.beta2 * self.v[param_id] + (1 - self.beta2) * (grads ** 2)\n",
        "        \n",
        "        # Compute bias-corrected first moment estimate\n",
        "        # m_hat_t = m_t / (1 - beta1^t)\n",
        "        m_hat = self.m[param_id] / (1 - self.beta1 ** t)\n",
        "        \n",
        "        # Compute bias-corrected second raw moment estimate\n",
        "        # v_hat_t = v_t / (1 - beta2^t)\n",
        "        v_hat = self.v[param_id] / (1 - self.beta2 ** t)\n",
        "        \n",
        "        # Update parameters\n",
        "        # w_t = w_{t-1} - lr * m_hat_t / (sqrt(v_hat_t) + eps)\n",
        "        params[:] = params - self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Numerical Example: Parameter Updates Over Several Steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example from requirements - exact code provided\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "W = np.random.randn(2, 2)\n",
        "grads = np.random.randn(2, 2)\n",
        "\n",
        "adam = Adam(lr=0.01)\n",
        "\n",
        "for i in range(5):\n",
        "    adam.step(W, grads)\n",
        "    print(f\"Step {i+1}, Updated Weights:\\n\", W)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detailed Example: Showing Intermediate Values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed example showing intermediate values (m_t, v_t, m_hat_t, v_hat_t)\n",
        "np.random.seed(42)\n",
        "W = np.random.randn(2, 2)\n",
        "grads = np.random.randn(2, 2)\n",
        "\n",
        "print(\"Initial weights W:\")\n",
        "print(W)\n",
        "print(\"\\nInitial gradients:\")\n",
        "print(grads)\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "adam = Adam(lr=0.01, beta1=0.9, beta2=0.999, eps=1e-8)\n",
        "\n",
        "param_id = id(W)\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"Step {i+1}:\")\n",
        "    print(f\"  Learning rate: {adam.lr}\")\n",
        "    print(f\"  Beta1: {adam.beta1}, Beta2: {adam.beta2}, Eps: {adam.eps}\")\n",
        "    \n",
        "    # Get current step count (will be used in this step)\n",
        "    current_t = adam.t.get(param_id, 0)\n",
        "    next_t = current_t + 1\n",
        "    print(f\"  Step count for this iteration (t): {next_t}\")\n",
        "    \n",
        "    # Store previous values for display\n",
        "    m_prev = adam.m.get(param_id, np.zeros_like(W)).copy()\n",
        "    v_prev = adam.v.get(param_id, np.zeros_like(W)).copy()\n",
        "    W_prev = W.copy()\n",
        "    \n",
        "    print(f\"\\n  Before step():\")\n",
        "    print(f\"    m_{i}: {m_prev}\")\n",
        "    print(f\"    v_{i}: {v_prev}\")\n",
        "    print(f\"    W_{i}: {W_prev}\")\n",
        "    print(f\"    Current gradients: {grads}\")\n",
        "    \n",
        "    # Perform step\n",
        "    adam.step(W, grads)\n",
        "    \n",
        "    # Now get updated values\n",
        "    t = adam.t[param_id]  # This is now next_t\n",
        "    \n",
        "    print(f\"\\n  After step() - Updated moments:\")\n",
        "    print(f\"    m_{i+1} = beta1 * m_{i} + (1 - beta1) * g\")\n",
        "    print(f\"    m_{i+1} = {adam.beta1} * {m_prev} + {1 - adam.beta1} * {grads}\")\n",
        "    print(f\"    m_{i+1}: {adam.m[param_id]}\")\n",
        "    print(f\"    v_{i+1} = beta2 * v_{i} + (1 - beta2) * g^2\")\n",
        "    print(f\"    v_{i+1} = {adam.beta2} * {v_prev} + {1 - adam.beta2} * {grads**2}\")\n",
        "    print(f\"    v_{i+1}: {adam.v[param_id]}\")\n",
        "    \n",
        "    # Bias correction\n",
        "    m_hat = adam.m[param_id] / (1 - adam.beta1 ** t)\n",
        "    v_hat = adam.v[param_id] / (1 - adam.beta2 ** t)\n",
        "    \n",
        "    print(f\"\\n  Bias correction (t={t}):\")\n",
        "    print(f\"    m_hat_{i+1} = m_{i+1} / (1 - beta1^{t})\")\n",
        "    print(f\"    m_hat_{i+1} = {adam.m[param_id]} / {1 - adam.beta1 ** t}\")\n",
        "    print(f\"    m_hat_{i+1}: {m_hat}\")\n",
        "    print(f\"    v_hat_{i+1} = v_{i+1} / (1 - beta2^{t})\")\n",
        "    print(f\"    v_hat_{i+1} = {adam.v[param_id]} / {1 - adam.beta2 ** t}\")\n",
        "    print(f\"    v_hat_{i+1}: {v_hat}\")\n",
        "    \n",
        "    print(f\"\\n  Parameter update:\")\n",
        "    denominator = np.sqrt(v_hat) + adam.eps\n",
        "    print(f\"    sqrt(v_hat_{i+1}) + eps: {denominator}\")\n",
        "    print(f\"    W_{i+1} = W_{i} - lr * m_hat_{i+1} / (sqrt(v_hat_{i+1}) + eps)\")\n",
        "    print(f\"    W_{i+1} = {W_prev} - {adam.lr} * {m_hat} / {denominator}\")\n",
        "    print(f\"    Updated W_{i+1}: {W}\")\n",
        "    print(\"-\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example: Adam with Weight Decay\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example demonstrating Adam with weight decay\n",
        "np.random.seed(42)\n",
        "\n",
        "W = np.random.randn(2, 2)\n",
        "grads = np.random.randn(2, 2)\n",
        "\n",
        "print(\"Adam without weight decay:\")\n",
        "adam_no_decay = Adam(lr=0.01, weight_decay=0.0)\n",
        "W_no_decay = W.copy()\n",
        "\n",
        "for i in range(3):\n",
        "    adam_no_decay.step(W_no_decay, grads)\n",
        "    print(f\"Step {i+1}, W:\\n{W_no_decay}\\n\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "print(\"Adam with weight decay (weight_decay=0.01):\")\n",
        "adam_with_decay = Adam(lr=0.01, weight_decay=0.01)\n",
        "W_with_decay = W.copy()\n",
        "\n",
        "for i in range(3):\n",
        "    adam_with_decay.step(W_with_decay, grads)\n",
        "    print(f\"Step {i+1}, W:\\n{W_with_decay}\\n\")\n",
        "    print(f\"  Weight decay contribution: {0.01 * W_with_decay}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step-by-Step Explanation\n",
        "\n",
        "### Adam Algorithm Overview\n",
        "\n",
        "Adam (Adaptive Moment Estimation) is an optimization algorithm that computes adaptive learning rates for each parameter.\n",
        "\n",
        "### Key Components:\n",
        "\n",
        "1. **First Moment (Momentum)**: Tracks the mean of gradients\n",
        "   - $m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$\n",
        "\n",
        "2. **Second Moment (Uncentered Variance)**: Tracks the mean of squared gradients\n",
        "   - $v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$\n",
        "\n",
        "3. **Bias Correction**: Since $m_t$ and $v_t$ are initialized to zero, they are biased towards zero, especially during early iterations. Bias correction accounts for this:\n",
        "   - $\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$\n",
        "   - $\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$\n",
        "\n",
        "4. **Parameter Update**: \n",
        "   - $\\theta_t = \\theta_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$\n",
        "   - Where $\\alpha$ is the learning rate and $\\epsilon$ is a small constant for numerical stability\n",
        "\n",
        "5. **Weight Decay (L2 Regularization)**: If enabled, adds penalty to gradients:\n",
        "   - $g_t = g_t + \\text{weight\\_decay} \\times \\theta_t$\n",
        "\n",
        "### Advantages:\n",
        "- Adaptive learning rate per parameter\n",
        "- Combines benefits of momentum and RMSProp\n",
        "- Good default hyperparameters work well for many problems\n",
        "- Bias correction makes early iterations more reliable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparison: Different beta values\n",
        "np.random.seed(42)\n",
        "\n",
        "W_1 = np.random.randn(2, 2)\n",
        "W_2 = np.random.randn(2, 2)\n",
        "grads = np.random.randn(2, 2)\n",
        "\n",
        "print(\"Adam with default betas (beta1=0.9, beta2=0.999):\")\n",
        "adam1 = Adam(lr=0.01, beta1=0.9, beta2=0.999)\n",
        "for i in range(3):\n",
        "    adam1.step(W_1, grads)\n",
        "    print(f\"Step {i+1}: {W_1}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "print(\"Adam with lower betas (beta1=0.5, beta2=0.5):\")\n",
        "adam2 = Adam(lr=0.01, beta1=0.5, beta2=0.5)\n",
        "for i in range(3):\n",
        "    adam2.step(W_2, grads)\n",
        "    print(f\"Step {i+1}: {W_2}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
