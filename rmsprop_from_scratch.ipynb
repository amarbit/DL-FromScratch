{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RMSProp Optimizer from Scratch\n",
        "\n",
        "This notebook implements the RMSProp (Root Mean Square Propagation) optimizer from scratch using only NumPy, including support for learning rate, decay factor (beta), numerical stability (eps), and optional weight decay (L2 regularization).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RMSProp Optimizer Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RMSProp:\n",
        "    \"\"\"\n",
        "    RMSProp (Root Mean Square Propagation) optimizer implemented from scratch using NumPy.\n",
        "    \n",
        "    RMSProp adapts the learning rate for each parameter by using a moving average of squared gradients.\n",
        "    This makes the algorithm effective in non-stationary settings and helps converge faster by\n",
        "    reducing the step size in directions with high gradient magnitudes.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    lr : float, optional\n",
        "        Learning rate (step size) for weight updates (default: 0.001)\n",
        "    beta : float, optional\n",
        "        Exponential decay rate for the moving average of squared gradients (default: 0.9)\n",
        "    eps : float, optional\n",
        "        Small constant added to the denominator for numerical stability (default: 1e-8)\n",
        "    weight_decay : float, optional\n",
        "        L2 regularization coefficient (default: 0.0)\n",
        "    \n",
        "    Algorithm:\n",
        "    ----------\n",
        "    1. Update moving average of squared gradients: v_t = β * v_{t-1} + (1 - β) * g_t^2\n",
        "    2. Update parameters: w_t = w_{t-1} - lr * g_t / (√v_t + ε)\n",
        "    \n",
        "    Examples:\n",
        "    --------\n",
        "    >>> optimizer = RMSProp(lr=0.01, beta=0.9, eps=1e-8, weight_decay=0.01)\n",
        "    >>> optimizer.step(weights, gradients)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, lr=0.001, beta=0.9, eps=1e-8, weight_decay=0.0):\n",
        "        self.lr = lr\n",
        "        self.beta = beta\n",
        "        self.eps = eps\n",
        "        self.weight_decay = weight_decay\n",
        "        \n",
        "        # Dictionary to store moving average of squared gradients for each parameter set\n",
        "        self.v = {}\n",
        "    \n",
        "    def step(self, params, grads):\n",
        "        \"\"\"\n",
        "        Perform a single RMSProp optimization step.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        params : numpy.ndarray\n",
        "            Model parameters (weights) to be updated (modified in-place)\n",
        "        grads : numpy.ndarray\n",
        "            Gradients with respect to the parameters\n",
        "        \n",
        "        Algorithm:\n",
        "        ----------\n",
        "        1. Apply weight decay if enabled: g_t = g_t + weight_decay * w_t\n",
        "        2. Update moving average of squared gradients: v_t = β * v_{t-1} + (1 - β) * g_t^2\n",
        "        3. Update parameters: w_t = w_{t-1} - lr * g_t / (√v_t + ε)\n",
        "        \"\"\"\n",
        "        # Get unique identifier for this parameter set\n",
        "        param_id = id(params)\n",
        "        \n",
        "        # Initialize moving average if not exists\n",
        "        if param_id not in self.v:\n",
        "            self.v[param_id] = np.zeros_like(params)\n",
        "        \n",
        "        # Apply L2 regularization (weight decay) to gradients\n",
        "        if self.weight_decay > 0:\n",
        "            grads = grads + self.weight_decay * params\n",
        "        \n",
        "        # Update moving average of squared gradients\n",
        "        # v_t = β * v_{t-1} + (1 - β) * g_t^2\n",
        "        self.v[param_id] = self.beta * self.v[param_id] + (1 - self.beta) * (grads ** 2)\n",
        "        \n",
        "        # Update parameters\n",
        "        # w_t = w_{t-1} - lr * g_t / (√v_t + ε)\n",
        "        params[:] = params - self.lr * grads / (np.sqrt(self.v[param_id]) + self.eps)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Numerical Example: Parameter Updates Over 5 Steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple numerical example\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "W = np.random.randn(2, 2)\n",
        "grads = np.random.randn(2, 2)\n",
        "\n",
        "print(\"Initial weights W:\")\n",
        "print(W)\n",
        "print(\"\\nInitial gradients:\")\n",
        "print(grads)\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# Create RMSProp optimizer\n",
        "rmsprop = RMSProp(lr=0.01, beta=0.9, eps=1e-8)\n",
        "\n",
        "print(\"RMSProp optimizer parameters:\")\n",
        "print(f\"  Learning rate: {rmsprop.lr}\")\n",
        "print(f\"  Beta (decay rate): {rmsprop.beta}\")\n",
        "print(f\"  Epsilon: {rmsprop.eps}\")\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# Perform 5 optimization steps\n",
        "for i in range(5):\n",
        "    rmsprop.step(W, grads)\n",
        "    print(f\"Step {i+1}, Updated Weights:\\n{W}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detailed Example: Showing Intermediate Values (v_t and Step Sizes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed example showing intermediate values (v_t and adaptive step sizes)\n",
        "np.random.seed(42)\n",
        "W = np.random.randn(2, 2)\n",
        "grads = np.random.randn(2, 2)\n",
        "\n",
        "print(\"Initial weights W:\")\n",
        "print(W)\n",
        "print(\"\\nInitial gradients:\")\n",
        "print(grads)\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "rmsprop = RMSProp(lr=0.01, beta=0.9, eps=1e-8)\n",
        "\n",
        "param_id = id(W)\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"Step {i+1}:\")\n",
        "    print(f\"  Learning rate: {rmsprop.lr}\")\n",
        "    print(f\"  Beta: {rmsprop.beta}, Eps: {rmsprop.eps}\")\n",
        "    \n",
        "    # Store previous values for display\n",
        "    v_prev = rmsprop.v.get(param_id, np.zeros_like(W)).copy()\n",
        "    W_prev = W.copy()\n",
        "    \n",
        "    print(f\"\\n  Before step():\")\n",
        "    print(f\"    v_{i}: {v_prev}\")\n",
        "    print(f\"    W_{i}: {W_prev}\")\n",
        "    print(f\"    Current gradients: {grads}\")\n",
        "    print(f\"    Gradient magnitudes: {np.abs(grads)}\")\n",
        "    \n",
        "    # Perform step\n",
        "    rmsprop.step(W, grads)\n",
        "    \n",
        "    # Display intermediate values\n",
        "    print(f\"\\n  After step() - Updated moving average:\")\n",
        "    print(f\"    v_{i+1} = beta * v_{i} + (1 - beta) * g_{i+1}^2\")\n",
        "    print(f\"    v_{i+1} = {rmsprop.beta} * {v_prev} + {1 - rmsprop.beta} * {grads**2}\")\n",
        "    print(f\"    v_{i+1}: {rmsprop.v[param_id]}\")\n",
        "    \n",
        "    # Calculate adaptive step sizes\n",
        "    sqrt_v = np.sqrt(rmsprop.v[param_id])\n",
        "    denominator = sqrt_v + rmsprop.eps\n",
        "    adaptive_lr = rmsprop.lr / denominator\n",
        "    step_size = adaptive_lr * grads\n",
        "    \n",
        "    print(f\"\\n  Adaptive step size calculation:\")\n",
        "    print(f\"    sqrt(v_{i+1}): {sqrt_v}\")\n",
        "    print(f\"    sqrt(v_{i+1}) + eps: {denominator}\")\n",
        "    print(f\"    Adaptive learning rate per parameter: {rmsprop.lr} / {denominator}\")\n",
        "    print(f\"    Adaptive learning rate: {adaptive_lr}\")\n",
        "    print(f\"    Step size: lr * g / (sqrt(v) + eps) = {step_size}\")\n",
        "    \n",
        "    print(f\"\\n  Parameter update:\")\n",
        "    print(f\"    W_{i+1} = W_{i} - lr * g_{i+1} / (sqrt(v_{i+1}) + eps)\")\n",
        "    print(f\"    Updated W_{i+1}: {W}\")\n",
        "    print(f\"    Change in W: {W - W_prev}\")\n",
        "    print(\"-\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example: Demonstrating Adaptive Learning Rates (Smaller Steps for High Gradients)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate how RMSProp adapts step sizes based on gradient magnitudes\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create a scenario with different gradient magnitudes\n",
        "W = np.array([[1.0, 2.0],\n",
        "              [3.0, 4.0]])\n",
        "\n",
        "# Gradients: large in first column, small in second column\n",
        "grads = np.array([[10.0, 0.1],\n",
        "                  [8.0,  0.2]])\n",
        "\n",
        "print(\"Demonstrating adaptive step sizes:\")\n",
        "print(f\"Initial weights W:\\n{W}\")\n",
        "print(f\"\\nGradients:\\n{grads}\")\n",
        "print(f\"Gradient magnitudes:\\n{np.abs(grads)}\")\n",
        "print(f\"\\nNote: Large gradients in first column, small in second column\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "rmsprop = RMSProp(lr=0.1, beta=0.9, eps=1e-8)\n",
        "\n",
        "for i in range(5):\n",
        "    v_before = rmsprop.v.get(id(W), np.zeros_like(W)).copy()\n",
        "    \n",
        "    # Store weights before update\n",
        "    W_before = W.copy()\n",
        "    \n",
        "    # Perform step\n",
        "    rmsprop.step(W, grads)\n",
        "    \n",
        "    # Calculate adaptive learning rates\n",
        "    sqrt_v = np.sqrt(rmsprop.v[id(W)])\n",
        "    adaptive_lr = rmsprop.lr / (sqrt_v + rmsprop.eps)\n",
        "    step_size = adaptive_lr * grads\n",
        "    \n",
        "    print(f\"Step {i+1}:\")\n",
        "    print(f\"  v (moving avg of squared gradients):\\n{rmsprop.v[id(W)]}\")\n",
        "    print(f\"  sqrt(v):\\n{sqrt_v}\")\n",
        "    print(f\"  Adaptive learning rate per parameter:\\n{adaptive_lr}\")\n",
        "    print(f\"  Step size per parameter:\\n{step_size}\")\n",
        "    print(f\"  Updated W:\\n{W}\")\n",
        "    print(f\"  Change in W:\\n{W - W_before}\")\n",
        "    print(f\"\\n  Key observation: Steps are smaller for parameters with larger gradients!\")\n",
        "    print(f\"  Column 1 (high gradients): avg step size = {np.mean(np.abs(step_size[:, 0])):.6f}\")\n",
        "    print(f\"  Column 2 (low gradients): avg step size = {np.mean(np.abs(step_size[:, 1])):.6f}\")\n",
        "    print(\"-\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example: RMSProp with Weight Decay\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example demonstrating RMSProp with weight decay\n",
        "np.random.seed(42)\n",
        "\n",
        "W = np.random.randn(2, 2)\n",
        "grads = np.random.randn(2, 2)\n",
        "\n",
        "print(\"RMSProp without weight decay:\")\n",
        "rmsprop_no_decay = RMSProp(lr=0.01, beta=0.9, weight_decay=0.0)\n",
        "W_no_decay = W.copy()\n",
        "\n",
        "for i in range(3):\n",
        "    rmsprop_no_decay.step(W_no_decay, grads)\n",
        "    print(f\"Step {i+1}, W:\\n{W_no_decay}\\n\")\n",
        "\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "print(\"RMSProp with weight decay (weight_decay=0.01):\")\n",
        "rmsprop_with_decay = RMSProp(lr=0.01, beta=0.9, weight_decay=0.01)\n",
        "W_with_decay = W.copy()\n",
        "\n",
        "for i in range(3):\n",
        "    rmsprop_with_decay.step(W_with_decay, grads)\n",
        "    print(f\"Step {i+1}, W:\\n{W_with_decay}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison: Different Beta Values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare RMSProp with different beta values\n",
        "np.random.seed(42)\n",
        "\n",
        "W_1 = np.random.randn(2, 2)\n",
        "W_2 = np.random.randn(2, 2)\n",
        "grads = np.random.randn(2, 2)\n",
        "\n",
        "print(\"RMSProp with beta=0.9 (default, slower decay):\")\n",
        "rmsprop1 = RMSProp(lr=0.01, beta=0.9)\n",
        "for i in range(3):\n",
        "    rmsprop1.step(W_1, grads)\n",
        "    print(f\"Step {i+1}: {W_1}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "print(\"RMSProp with beta=0.5 (faster decay, more responsive to recent gradients):\")\n",
        "rmsprop2 = RMSProp(lr=0.01, beta=0.5)\n",
        "for i in range(3):\n",
        "    rmsprop2.step(W_2, grads)\n",
        "    print(f\"Step {i+1}: {W_2}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step-by-Step Explanation\n",
        "\n",
        "### RMSProp Algorithm Overview\n",
        "\n",
        "RMSProp (Root Mean Square Propagation) is an adaptive learning rate optimization algorithm that maintains a moving average of squared gradients. This allows it to adaptively adjust the learning rate for each parameter.\n",
        "\n",
        "### Key Components:\n",
        "\n",
        "1. **Moving Average of Squared Gradients**:\n",
        "   - $v_t = \\beta v_{t-1} + (1 - \\beta) g_t^2$\n",
        "   - Tracks the magnitude of gradients over time\n",
        "   - $\\beta$ controls how quickly old information decays (typically 0.9)\n",
        "\n",
        "2. **Parameter Update**:\n",
        "   - $\\theta_t = \\theta_{t-1} - \\alpha \\frac{g_t}{\\sqrt{v_t} + \\epsilon}$\n",
        "   - Where $\\alpha$ is the learning rate and $\\epsilon$ is a small constant for numerical stability\n",
        "   - Divides gradients by the square root of the moving average, effectively normalizing step sizes\n",
        "\n",
        "3. **Weight Decay (L2 Regularization)**: If enabled, adds penalty to gradients:\n",
        "   - $g_t = g_t + \\text{weight\\_decay} \\times \\theta_t$\n",
        "\n",
        "### Advantages:\n",
        "\n",
        "- **Adaptive Learning Rates**: Automatically adjusts step size per parameter\n",
        "- **Handles Non-Stationary Objectives**: Works well when gradients vary significantly over time\n",
        "- **Smoother Convergence**: Reduces oscillations by damping high-gradient directions\n",
        "- **Smaller Steps for High Gradients**: Parameters with consistently large gradients get smaller step sizes, promoting stability\n",
        "\n",
        "### Key Insight:\n",
        "\n",
        "The algorithm divides the gradient by the square root of the moving average of squared gradients. This means:\n",
        "- If a parameter has consistently large gradients, $v_t$ will be large, making the step size smaller\n",
        "- If a parameter has consistently small gradients, $v_t$ will be small, making the step size relatively larger (but still bounded by the learning rate)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
