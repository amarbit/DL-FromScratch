{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (SGD) from Scratch\n",
    "\n",
    "This notebook implements Stochastic Gradient Descent (SGD) optimizer from scratch using only NumPy, including support for momentum and weight decay (L2 regularization).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Optimizer Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent (SGD) optimizer implemented from scratch using NumPy.\n",
    "    \n",
    "    Supports:\n",
    "    - Vanilla SGD (when momentum=0 and weight_decay=0)\n",
    "    - Momentum SGD\n",
    "    - L2 regularization (weight decay)\n",
    "    - Combined momentum and weight decay\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lr : float\n",
    "        Learning rate for weight updates (default: 0.01)\n",
    "    momentum : float, optional\n",
    "        Momentum factor. When > 0, uses momentum SGD (default: 0.0)\n",
    "    weight_decay : float, optional\n",
    "        L2 regularization coefficient (default: 0.0)\n",
    "    \n",
    "    Examples:\n",
    "    --------\n",
    "    >>> optimizer = SGD(lr=0.1, momentum=0.9, weight_decay=0.01)\n",
    "    >>> optimizer.step(weights, gradients)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr=0.01, momentum=0.0, weight_decay=0.0):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.velocity = {}  # Dictionary to store velocity for each parameter set\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        \"\"\"\n",
    "        Perform a single SGD optimization step.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        params : numpy.ndarray\n",
    "            Model parameters (weights) to be updated (modified in-place)\n",
    "        grads : numpy.ndarray\n",
    "            Gradients with respect to the parameters\n",
    "        \n",
    "        Notes:\n",
    "        ------\n",
    "        - If weight_decay > 0, L2 regularization is applied: g_i = g_i + weight_decay * w_i\n",
    "        - If momentum > 0, momentum update is used:\n",
    "          v_i = momentum * v_i - lr * g_i\n",
    "          w_i = w_i + v_i\n",
    "        - Otherwise, vanilla SGD: w_i = w_i - lr * g_i\n",
    "        \"\"\"\n",
    "        # Get unique identifier for this parameter set (based on memory address)\n",
    "        param_id = id(params)\n",
    "        \n",
    "        # Apply L2 regularization (weight decay) to gradients\n",
    "        if self.weight_decay > 0:\n",
    "            grads = grads + self.weight_decay * params\n",
    "        \n",
    "        # Apply momentum if enabled\n",
    "        if self.momentum > 0:\n",
    "            # Initialize velocity if not exists\n",
    "            if param_id not in self.velocity:\n",
    "                self.velocity[param_id] = np.zeros_like(params)\n",
    "            \n",
    "            # Update velocity: v_i = momentum * v_i - lr * g_i\n",
    "            self.velocity[param_id] = self.momentum * self.velocity[param_id] - self.lr * grads\n",
    "            \n",
    "            # Update parameters: w_i = w_i + v_i\n",
    "            params[:] = params + self.velocity[param_id]\n",
    "        else:\n",
    "            # Vanilla SGD: w_i = w_i - lr * g_i\n",
    "            params[:] = params - self.lr * grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Example: Weight Updates Over Iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Updated W:\n",
      " [[1.57553249 0.49748484]\n",
      " [0.8827504  2.25378803]]\n",
      "Iteration 2, Updated W:\n",
      " [[1.2175333  0.68231001]\n",
      " [0.70046999 2.2782753 ]]\n",
      "Iteration 3, Updated W:\n",
      " [[0.70736069 0.94569814]\n",
      " [0.44070831 2.3131713 ]]\n",
      "Iteration 4, Updated W:\n",
      " [[0.06074218 1.27952955]\n",
      " [0.11147324 2.35740025]]\n",
      "Iteration 5, Updated W:\n",
      " [[-0.70803102  1.67642608]\n",
      " [-0.27995863  2.40998462]]\n"
     ]
    }
   ],
   "source": [
    "# Example from requirements - exact code provided\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "W = np.random.randn(2, 2)\n",
    "grads = np.random.randn(2, 2)\n",
    "\n",
    "sgd = SGD(lr=0.1, momentum=0.9, weight_decay=0.01)\n",
    "\n",
    "for i in range(5):\n",
    "    sgd.step(W, grads)\n",
    "    print(f\"Iteration {i+1}, Updated W:\\n\", W)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Comparing Vanilla SGD vs Momentum SGD vs Weight Decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights (all methods start with same values):\n",
      "[[ 0.49671415 -0.1382643 ]\n",
      " [ 0.64768854  1.52302986]]\n",
      "\n",
      "============================================================\n",
      "\n",
      "1. Vanilla SGD (lr=0.1, momentum=0, weight_decay=0):\n",
      "   After iteration 1:\n",
      "   [[ 0.52012949 -0.11485061]\n",
      " [ 0.48976726  1.44628638]]\n",
      "\n",
      "2. Momentum SGD (lr=0.1, momentum=0.9, weight_decay=0):\n",
      "   After iteration 1:\n",
      "   [[ 0.52012949 -0.11485061]\n",
      " [ 0.48976726  1.44628638]]\n",
      "\n",
      "3. Weight Decay SGD (lr=0.1, momentum=0, weight_decay=0.01):\n",
      "   After iteration 1:\n",
      "   [[ 0.51963278 -0.11471234]\n",
      " [ 0.48911957  1.44476335]]\n",
      "\n",
      "4. Momentum + Weight Decay (lr=0.1, momentum=0.9, weight_decay=0.01):\n",
      "   After iteration 1:\n",
      "   [[ 0.51963278 -0.11471234]\n",
      " [ 0.48911957  1.44476335]]\n"
     ]
    }
   ],
   "source": [
    "# Demonstration comparing different SGD variants\n",
    "np.random.seed(42)\n",
    "\n",
    "# Same initial conditions for fair comparison\n",
    "W_vanilla = np.random.randn(2, 2)\n",
    "W_momentum = W_vanilla.copy()\n",
    "W_decay = W_vanilla.copy()\n",
    "W_both = W_vanilla.copy()\n",
    "\n",
    "grads = np.random.randn(2, 2)\n",
    "\n",
    "print(\"Initial weights (all methods start with same values):\")\n",
    "print(W_vanilla)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Vanilla SGD\n",
    "print(\"1. Vanilla SGD (lr=0.1, momentum=0, weight_decay=0):\")\n",
    "sgd_vanilla = SGD(lr=0.1, momentum=0.0, weight_decay=0.0)\n",
    "for i in range(3):\n",
    "    sgd_vanilla.step(W_vanilla, grads)\n",
    "    if i == 0:\n",
    "        print(f\"   After iteration {i+1}:\")\n",
    "        print(f\"   {W_vanilla}\")\n",
    "\n",
    "# Momentum SGD\n",
    "print(\"\\n2. Momentum SGD (lr=0.1, momentum=0.9, weight_decay=0):\")\n",
    "sgd_momentum = SGD(lr=0.1, momentum=0.9, weight_decay=0.0)\n",
    "for i in range(3):\n",
    "    sgd_momentum.step(W_momentum, grads)\n",
    "    if i == 0:\n",
    "        print(f\"   After iteration {i+1}:\")\n",
    "        print(f\"   {W_momentum}\")\n",
    "\n",
    "# Weight Decay SGD\n",
    "print(\"\\n3. Weight Decay SGD (lr=0.1, momentum=0, weight_decay=0.01):\")\n",
    "sgd_decay = SGD(lr=0.1, momentum=0.0, weight_decay=0.01)\n",
    "for i in range(3):\n",
    "    sgd_decay.step(W_decay, grads)\n",
    "    if i == 0:\n",
    "        print(f\"   After iteration {i+1}:\")\n",
    "        print(f\"   {W_decay}\")\n",
    "\n",
    "# Both Momentum and Weight Decay\n",
    "print(\"\\n4. Momentum + Weight Decay (lr=0.1, momentum=0.9, weight_decay=0.01):\")\n",
    "sgd_both = SGD(lr=0.1, momentum=0.9, weight_decay=0.01)\n",
    "for i in range(3):\n",
    "    sgd_both.step(W_both, grads)\n",
    "    if i == 0:\n",
    "        print(f\"   After iteration {i+1}:\")\n",
    "        print(f\"   {W_both}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Explanation\n",
    "\n",
    "### 1. L2 Regularization (Weight Decay)\n",
    "When `weight_decay > 0`, the gradient is modified:\n",
    "```\n",
    "g_i = g_i + weight_decay × w_i\n",
    "```\n",
    "This adds a penalty term proportional to the weight magnitude, encouraging smaller weights.\n",
    "\n",
    "### 2. Momentum Update\n",
    "When `momentum > 0`, the update rule becomes:\n",
    "```\n",
    "v_i = momentum × v_i - lr × g_i\n",
    "w_i = w_i + v_i\n",
    "```\n",
    "This helps accelerate convergence and reduce oscillations.\n",
    "\n",
    "### 3. Vanilla SGD\n",
    "When `momentum = 0` and `weight_decay = 0`:\n",
    "```\n",
    "w_i = w_i - lr × g_i\n",
    "```\n",
    "Simple gradient descent update.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-by-step calculation:\n",
      "\n",
      "Initial W:\n",
      "[[1.76405235 0.40015721]\n",
      " [0.97873798 2.2408932 ]]\n",
      "\n",
      "Initial gradients:\n",
      "[[ 1.86755799 -0.97727788]\n",
      " [ 0.95008842 -0.15135721]]\n",
      "\n",
      "--- Before step() ---\n",
      "Weight decay: 0.01\n",
      "Momentum: 0.9\n",
      "Learning rate: 0.1\n",
      "\n",
      "Gradients after weight decay:\n",
      "[[ 1.88519851 -0.97327631]\n",
      " [ 0.9598758  -0.12894828]]\n",
      "  (grads + 0.01 * W)\n",
      "\n",
      "Initial velocity:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "\n",
      "Updated velocity:\n",
      "[[-0.18851985  0.09732763]\n",
      " [-0.09598758  0.01289483]]\n",
      "  (momentum * v - lr * g)\n",
      "  (0.9 * v - 0.1 * g)\n",
      "\n",
      "New weights:\n",
      "[[1.57553249 0.49748484]\n",
      " [0.8827504  2.25378803]]\n",
      "  (W + velocity)\n",
      "\n",
      "--- After step() ---\n",
      "Final W:\n",
      "[[1.57553249 0.49748484]\n",
      " [0.8827504  2.25378803]]\n"
     ]
    }
   ],
   "source": [
    "# Detailed step-by-step calculation for one iteration\n",
    "np.random.seed(0)\n",
    "\n",
    "# Initialize\n",
    "W = np.random.randn(2, 2)\n",
    "grads = np.random.randn(2, 2)\n",
    "\n",
    "print(\"Step-by-step calculation:\")\n",
    "print(f\"\\nInitial W:\\n{W}\")\n",
    "print(f\"\\nInitial gradients:\\n{grads}\")\n",
    "\n",
    "# Create optimizer\n",
    "sgd = SGD(lr=0.1, momentum=0.9, weight_decay=0.01)\n",
    "\n",
    "# Show intermediate steps\n",
    "param_id = id(W)\n",
    "print(f\"\\n--- Before step() ---\")\n",
    "print(f\"Weight decay: {sgd.weight_decay}\")\n",
    "print(f\"Momentum: {sgd.momentum}\")\n",
    "print(f\"Learning rate: {sgd.lr}\")\n",
    "\n",
    "# Apply weight decay manually for demonstration\n",
    "if sgd.weight_decay > 0:\n",
    "    grads_with_decay = grads + sgd.weight_decay * W\n",
    "    print(f\"\\nGradients after weight decay:\\n{grads_with_decay}\")\n",
    "    print(f\"  (grads + {sgd.weight_decay} * W)\")\n",
    "else:\n",
    "    grads_with_decay = grads\n",
    "\n",
    "# Apply momentum manually for demonstration\n",
    "if sgd.momentum > 0:\n",
    "    # Initialize velocity (as done in step method)\n",
    "    if param_id not in sgd.velocity:\n",
    "        sgd.velocity[param_id] = np.zeros_like(W)\n",
    "    print(f\"\\nInitial velocity:\\n{sgd.velocity[param_id]}\")\n",
    "    \n",
    "    # Update velocity\n",
    "    sgd.velocity[param_id] = sgd.momentum * sgd.velocity[param_id] - sgd.lr * grads_with_decay\n",
    "    print(f\"\\nUpdated velocity:\\n{sgd.velocity[param_id]}\")\n",
    "    print(f\"  (momentum * v - lr * g)\")\n",
    "    print(f\"  ({sgd.momentum} * v - {sgd.lr} * g)\")\n",
    "    \n",
    "    W_new = W + sgd.velocity[param_id]\n",
    "    print(f\"\\nNew weights:\\n{W_new}\")\n",
    "    print(f\"  (W + velocity)\")\n",
    "\n",
    "# Now call the actual step method for verification\n",
    "np.random.seed(0)\n",
    "W_actual = np.random.randn(2, 2)\n",
    "sgd_actual = SGD(lr=0.1, momentum=0.9, weight_decay=0.01)\n",
    "sgd_actual.step(W_actual, grads)\n",
    "print(f\"\\n--- After step() ---\")\n",
    "print(f\"Final W:\\n{W_actual}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
